[{"authors":["admin"],"categories":null,"content":"Byung Il Choi is working on a football simulation software at Tict.ai. His work simulates football, soccer in US, matches which incorporates AI agents.\nHe studied Computer Science at the University of Illinois at Urbana-Champaign. He's interested in reinforcement learning and the machine augmented intelligence which is related to information technology and AI.\nHis undergraduate researches were advised by Prof. Cheng Xiang Zhai for Information Retrieval, and advised by Prof. Eric Shaffer for VR, AI, and Computer Graphics projects.\n","date":1575158400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1575158400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Byung Il Choi is working on a football simulation software at Tict.ai. His work simulates football, soccer in US, matches which incorporates AI agents.\nHe studied Computer Science at the University of Illinois at Urbana-Champaign. He's interested in reinforcement learning and the machine augmented intelligence which is related to information technology and AI.\nHis undergraduate researches were advised by Prof. Cheng Xiang Zhai for Information Retrieval, and advised by Prof. Eric Shaffer for VR, AI, and Computer Graphics projects.","tags":null,"title":"Byung Il Choi","type":"authors"},{"authors":null,"categories":null,"content":"GOAT is under development.\n","date":1600128000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600128000,"objectID":"7e72c0dbdbcf5ebd780425f9ff35c6c5","permalink":"/project/goat/","publishdate":"2020-09-15T00:00:00Z","relpermalink":"/project/goat/","section":"project","summary":"Football match simulation RL/AI","tags":["AI","Reinforcement Learning","Football","--"],"title":"GOAT","type":"project"},{"authors":["Byung Il Choi","Eric Shaffer (Advisor)"],"categories":null,"content":"Currently In Progress\n","date":1575158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575158400,"objectID":"445328289d8b39061d855be8ac8b7837","permalink":"/publication/proganterrain/","publishdate":"2019-12-01T00:00:00Z","relpermalink":"/publication/proganterrain/","section":"publication","summary":"Progressive Growing GAN used for Terrain Generation","tags":["Deep Learning","ProGAN","GAN","WGAN-GP","OpenGL","Terrain"],"title":"Progressive Growing GAN for Terrain Generation","type":"publication"},{"authors":null,"categories":null,"content":"Introduction As terrain generation would require a resolution that is higher than 100, DCGAN may not be a good fit for the purpose. This project has adapted Progressive Growing GAN for stable training and generations in a higher resolution. For more detail, there's a technical draft available.\nProgressive Growing GAN Progressive Growing GAN uses He initialization, pixelwise normalization, minibatch standard deviation, equalized learning rate for convolutions, exponential running average, and blending layers with different resolutions. This fading technique helps the network to get stablized. The MNIST dataset was used in early stage to make sure that the network works well as shown in Misc section.\nTraining The neural network was trained on the topology data and surface color from NASA's Visible Earth.\nThe training process combines the colors of the earth's surface and it's topology, which enables the network to generate both colors and heights of a terrain like below:\n    generated terrain colors      generated heights   These generated samples can be visualized in 3D.\n3.1 Terrain Visualization in 3D The below video shows how the generated terrains can be visualized in 3D.\n  The generations from fixed random latent vectors were tracked, and their training progress in 3D representations were captured as below:\n3.2 Training Visualization in 3D We can see how the GAN is combining various features in the visualization, which we cannot see in the 2D visualization.\n  3.2 Training Visualization in 2D The left half is how surface colors are generated and the right half shows the corresponding heights.\n  Acknowledgement This research project was advised by Prof. Eric Shaffer.\nMisc. MNIST digits generation using Progressive Growing GAN\n","date":1573948800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573948800,"objectID":"47e86858c4f5b6fa2b242b5054fcf196","permalink":"/project/terrain-gan/","publishdate":"2019-11-17T00:00:00Z","relpermalink":"/project/terrain-gan/","section":"project","summary":"Terrain Generation using Progressive Growing GAN","tags":["Deep Learning","ProGAN","GAN","WGAN-GP","OpenGL","Terrain","AI","--"],"title":"Terrain Generation with GAN","type":"project"},{"authors":["Byung Il Choi"],"categories":[],"content":"While building and testing a progressive growing GAN, I was able to train the network with the MNIST dataset. The results looks good and the training visualization shows how Progressive Growing GAN uses multiple layers to increase the resolution as below:\n  Acknowledgement  MNIST Dataset is from http://yann.lecun.com/exdb/mnist/ Progressive Growing GAN is implemented using PyTorch.  ","date":1573171200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573171200,"objectID":"961c8c29ac8ab6b7ad0d6dd4861e8556","permalink":"/post/mnist-progan/","publishdate":"2019-11-08T00:00:00Z","relpermalink":"/post/mnist-progan/","section":"post","summary":"Generating digits works well, model is trained from MNIST dataset","tags":["GAN","ProGAN","MNIST"],"title":"ProGAN on Generating Digits from MNIST","type":"post"},{"authors":["Byung Il Choi","Cheng Xiang Zhai (Advisor)"],"categories":null,"content":"Currently In Progress\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"aa6ebe7f1a0f3948340266d610eb02fe","permalink":"/publication/gqe_timan/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/publication/gqe_timan/","section":"publication","summary":"Generative Query Expansion","tags":["Information Retrieval","Query Expansion","GQE"],"title":"Generative Query Expansion, Stochastic Approach with Quantitative Analysis","type":"publication"},{"authors":null,"categories":null,"content":"Introduction The reinforcement learning has been around for more than a decade [2] , and Q-learning is one of most famous method for RL. DeepMind has especially utilized Q-learning, and evolved the idea to the next level by adapting non-linear model. Instead of Q table, Neural network is now used to predict the values, which is called Deep Q Network. The CNN and Deep Q-learning is what DeepMind’s work is all about.\nProblem DQN has to observe a game from ATARI and be trained to beat human level performance. Since Space Invader was the main example of early Deep-Mind’s paper, we are going to chose the game for this project. We are going to train our model to achieve the score of 500, which is the result of \u0026ldquo;Playing atari with deep reinforcement learn- ing.\u0026quot;, the first DeepMind’s paper. And Space Invader is a game that human level performance is hard to achieve in the early works of DQN.\nRelated Work DeepMind’s two early papers has two versions; we chose the version that uses three convolutional layers and one FC layer with 512 hidden units. This particular model for Space Invader has 6 outputs. All of the layers are followed by ReLU except the last output layer.\nTarget Network Because of the shifting weights of Q model, DeepMind has suggested to use two Q networks. This is related to divergence of Reinforcement Learning. The first network consistently updates as normal DNN would, but the second network, called the target network, doesn’t have sequential updates but is used to estimate the value of next state. Because of this it’s updated every 10,000 frames, but this can be changed through hyperparameter settings. According to the Deep-Mind’s paper, target network prevents of falling into a bad local minima and shifting weights. Thus, the target network has different set of parameters θ.\nExperience Replay Our problem includes correlations of sequences of input which consists of actions and states. In order to break this, DeepMind suggests to use Experience Replay. Experience Replay is another word for big sampling pool. The continuous sequences of data can be recorded to become a large pool, which can be used for sampling later. As we get more and more data, having correlation between sampled batch is more unlikely. Thereby, sampled batchs can be treated as if they are independent themselves.\nDQL Agent playing Space Invaders   Acknowledgement  This project was advised by Prof. Justin Sirignano. This project was tested and reviewed by Seongsu Ha  ","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"fea8bca9900f78441d6e9da047c827cc","permalink":"/project/dql-atari/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/project/dql-atari/","section":"project","summary":"DQL used to play Atari 2600 games","tags":["DQL","Q Learning","AI","Deep Learning","--"],"title":"Deep Q-Learning for Atari Games","type":"project"},{"authors":null,"categories":null,"content":"1. Introduction An AI agent is often not tangible. It’s an artificially being that usually resides in virtual space. We can seldom see robots and drones with AI on media, but not usually in the physical world. It’s very hard to bring agents to the physical world, but if we let them stay in their space, it becomes a lot easier to interact. The VR technologies let us be in a virtual space, thus we can experience the embodiment of an AI agent. This project is motivated by this gateway to space of another being.\n2. Problem It’s very costly to bring AI to physical space, and it would cost a tremendous amount to develop and test such agent in physical space. But, if we are able to simulate good enough for AI agents, we believe that the cost of development can be reduced drastically. And, we believe the AI agent in VR can be very entertaining.\n3. Practices For comfortable experience, OBP suggests few things. And, these are the main guideline that we will follow:\n(i) distance to focusing object is between 0.75 and 3.5 meters [OBP 10],\n(ii) post-processing effects are applied to both eyes [OBP 10],\n(iii) use default FOV [OBP 13],\n(iv) Viewing the environment from a stationary position is most comfortable. [OBP 6] Users will mostly be standing in a training-stage, and evaluation stage in a game form would also mostly be in a stationary position.\n4. Rules for Agents All types of agents have 30 decisions/sec and moves in same speed except human players. When the ball bounces, its reflection angle and speed randomly change, so that its harder for the AI agents. Without continuous random adjustments, Q-Agents can play the game for very long time. It's designed to test their limits.\n5-1 Q-Learning Agents Playing each other   Most of the mistakes are by movements that are off by an inch. The high ball speed can reduce the game time effectively.\n5-2 Q-Agent vs Greedy Agent   A greedy agent can never beat a Q-Learning agent, even though they have same decision making rates and moving speed.\nAcknowledgement This project was advised by Prof. Eric Shaffer.\n AI, Physics, and Graphics Programming by Byung Il Choi Graphics Programming and Game Play by Seongsu Ha User Interface and Graphic Design by Chris Wegenek Sound and Testing by Alejandro Marin  ","date":1545868800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545868800,"objectID":"0e31491c4fcb4615be5761f11e95cf90","permalink":"/project/q-learning-vr/","publishdate":"2018-12-27T00:00:00Z","relpermalink":"/project/q-learning-vr/","section":"project","summary":"To experience AI Agents in VR","tags":["AI","Reinforcement Learning","Unity","Q Learning","--"],"title":"Q-Learning Agent in VR","type":"project"},{"authors":null,"categories":null,"content":"Introduction In many query expansion studies, the primary evaluation depends on its search result. This project tries to provide a second way1 of measuring QE performance by measuring how many query expansion terms overlaps with ideal expansion terms, where ideal expansion terms are trained from a high performing QE from supervised learning. After expansion, we used Okapi BM25 for ranking(searching).\nApproaches (i) Try to find expansion cadidates using GQE model which tries find the terms that maximizes the probability of coocurrening terms.\n(ii) Try to see if maximizing probabilities of coocurring terms is the right way\n(iii) Try to find cohesiveness of coocurring terms\nFinding Ideal Expansion Terms We need to find ideal expansion terms first, so we have a criterion for comparing model's ouput. A supervised learning can find expansion terms that performs search well as it gives a result with high P@10 and MAP. The ideal expansion from training on AP88/89 performs as following:\n   Expansion p@5 p@10 MAP@10 MAP@100 MAP@1k Recall@1k Recall@10k     +0 terms 0.41 0.371 0.28 \u0026lt; 0.25 \u0026lt; 0.25 0.662 0.861   +400 0.906 0.827 0.817 0.556 0.546 0.8578 0.9642   +2000 0.98 0.959 0.9832 0.8734 0.7935 0.8825 0.9495    In early work, the goal was to find 400-expansion terms. Later, we found that 400 was not the upper bound but 2000+ terms could be used for expansion.\nEarly Work The first approach assumes that 400 expansion terms are enough quantity for QE. The top-100 terms that maxmizes the probability of coocurrence decides high amount of MAP. So, finding these top-100 terms are important as top-100 contributes most to the maximization. GQE finds top 100 as below:\n   generated terrain colors   The above graph is in query level measurment which shows how many exansion terms could be found that are in top-100 ideal expansion terms.\nThe larger model finds more ideal expansion terms, but so is the false positive expansion candidates.\nQuality vs Quantity The early work tries find terms that maximizes the probability of coocurrence, and it focused on top 100 terms. During the process, we found that the number of expandable terms can be more than 1000 and it gives a much better result as we see below:\n   Expansion p@5 p@10 MAP@10 MAP@100 MAP@1k     +400 0.906 0.827 0.817 0.556 0.546   +2000 0.98 0.959 0.9832 0.8734 0.7935    Let's use Monte Carlo method to make sure that the quantity matters more than quality; we randomly pick n terms from the ideal expansion term pool that we got from supervised learning, and see how it performs below:\n    Max Cooc 400  Monte Carlo @400 Monte Carlo @500 Monte Carlo @600 Monte Carlo @700 Monte Carlo @800 Monte Carlo @900 Monte Carlo @1000      MAP@100: 0.556  0.568 0.614 0.653 0.682 0.704 0.725 0.742     Max Cooc is using 400 expansion terms which come from maximized term co-occurence probability; this is no better than QE with 400 randomly picked expansion terms from ideal expansion pool as shown in Monte Carlo@400 method. As random picks are better than the co-occurence proability maximization method, it's better to focus on finding terms quantitatively.\nRecent Work The idea of finding top 100 ideal expansion is not the right direction as we found that 400 ideal expansion terms were at its local maxima for its performance. The quantity of expansion term mattered more than the quality, where the quality is high when the ideal expansion terms maximizes the co-occurrence probability of the expansion terms.\nGQE seems to capture a lot of ideal expansion terms:\n   GQE first n-terms 0-50 0-100 0-200 0-300 0-400 0-Max     Avg ideal terms found 41 82 161 236 304 1156   Precision 0.82 0.82 0.805 07867 0.76 n/a    The precision of finding these ideal expansion terms with GQE is quite high, but its error is not low enough yet. But, with appropriate terms injected, GQE's extension can be useful. In the first feedback loop, we get enough positive terms that can lower the error rates of GQE. With one feedback and GQE expansion, the search results improves as below:\n    No GQE GQE+50 GQE+100 GQE+150 GQE+200 GQE+250 GQE+300 GQE+350     p@5 0.41 0.496 0.506 0.516 0.522 0.526 0.534 0.514   p@10 0.371 0.391 0.404 0.411 0.417 0.428 0.42 0.415    GQE+n is for queries expanded with n-GQE terms and 1 feedback. The injection of true positive terms from a feedback helps to reduce GQE's error rate, and improves the search performance.\nConclusion So far, we've found that GQE captures a great portion of ideal expansion terms and is useful when it's used with the first feedback. If we find a way to increase the precision of finding GQE candidates(ie. filter out false positive terms), then the query expansion will improve even more as GQE's candidate terms are becoming closer to ideal expansion terms.\nAcknowledgement This project is advised by Prof. Cheng Xiang Zhai\n  This project is currently under progress. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1510704000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1510704000,"objectID":"8f0ba94cc2e9eb6a61516188072bbe8b","permalink":"/project/gqe-timan/","publishdate":"2017-11-15T00:00:00Z","relpermalink":"/project/gqe-timan/","section":"project","summary":"Stochastic Approach and Quantitative Analysis","tags":["Information Retrieval","Query Expansion","BM25","--"],"title":"Generative Query Expansion","type":"project"},{"authors":null,"categories":null,"content":"1. Visualizing Mandelbrot set in realtime Mandelbrot set can be visualized using OpenGL and cuda together as above. If this is rendered using CPU1, its FPS is as below:\n   CPU usage   There is no parallelization with the base CPU model. Procedural double loop and heavy calculation is very slow that causes the rendering to become uninteractive. This is, however, very a good subject for the GPU parallel computation. Because, it has a small amount of branchings and rare memory access. Each pixel will have independency for computation in lots of cases.\n2. Using GPU for realtime visualization Converting double loop to a single loop is simple in case of rendering mandelbrot set, since PBO has a linear memory space. Similar to parallel histogram computation, the distrbution of computation can be easily done with GPU model.\nUsing proper amount of threads boosts the speed as following:\n   GPU usage   The left shows a kernel with 49x1024 threads, which boosts about 333 times compared to the base model. With proper thread-block dimension, 15% more improvement can be acheived.\n3. n-stream rendering The above kernel in section 2 renders about 42 pixels per thread 2, where each thread's memory access will be about 168 bytes apart with no global memory access. Even for writing, it is better to coalesce the memory access for a better cache use. Instead of reindexing the writing process, changing the calculation level to per-pixel base can help for speeding up the calculation with a parallelization. Using 42 streams with 376x128 threads can speed up even more as below:\n   GPU speedup with streams   This is about 650 times faster than the base CPU model. The use of shared memory could not speed up the process since the GPU model didn't depend on previously calculated values. The realtime visualization of Mandelbrot set was a problem for how to distribute the computation than how to optimize the kernel.\n4. Realtime Visualization The GPU model can be visualized in realtime as below:   Acknowledgement This project was advised by Prof. Klaus Mueller.\nFootnotes   using AMD 8300 in 2015 \u0026#x21a9;\u0026#xfe0e;\n using Nvidia 780 gtx \u0026#x21a9;\u0026#xfe0e;\n   ","date":1430092800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1430092800,"objectID":"9d27af599fde21903978b103b17c05ba","permalink":"/project/mandelbrotcuda/","publishdate":"2015-04-27T00:00:00Z","relpermalink":"/project/mandelbrotcuda/","section":"project","summary":"Zooming into Mandelbrot set","tags":["Cuda","Fractal","Visualization","Graphics","Mandelbrot","--"],"title":"Realtime Fractal Zooming with CUDA","type":"project"},{"authors":null,"categories":null,"content":"2009 Finger Touch Tracking R\u0026amp;D: Worked on a project for finger tracking with Bezier Forward Difference\n2008 Adjustable Game Probabilities with Constraints R\u0026amp;D: Worked on adjustable game probability in Impression Co.\n2006 Camera Work and Shader Computation R\u0026amp;D: Worked on Camera Work related to curve tracking and Shader Computation\n2003 Scene Graph Library, Maya Plugin R\u0026amp;D: Worked on building scene graph library and related Maya Plugins in Cenozoic Co.\n2001 Skeleton Retargeting and Motion Capture Reprocessing R\u0026amp;D: Worked on motion captured data reprocessing in Cenozoic Co.\n1998 Text MUD Worked on Text MUD game for personal enjoyment\n","date":1264550400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1264550400,"objectID":"10533d53f9e52a8f35bc58e449192bd1","permalink":"/project/etc/","publishdate":"2010-01-27T00:00:00Z","relpermalink":"/project/etc/","section":"project","summary":"Other old projects","tags":["project"],"title":"Etc","type":"project"}]