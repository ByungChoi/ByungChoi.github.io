[{"authors":["admin"],"categories":null,"content":"Byung Il Choi is a senior studying Computer Science at the University of Illinois at Urbana-Champaign. He's interested in the machine augmented intelligence which is related to information technology and AI.\nHis undergraduate researches are advised by Prof. Cheng Xiang Zhai for Information Retrieval, and advised by Prof. Eric Shaffer for VR, AI, and Computer Graphics projects.\n","date":1575158400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1575158400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Byung Il Choi is a senior studying Computer Science at the University of Illinois at Urbana-Champaign. He's interested in the machine augmented intelligence which is related to information technology and AI.\nHis undergraduate researches are advised by Prof. Cheng Xiang Zhai for Information Retrieval, and advised by Prof. Eric Shaffer for VR, AI, and Computer Graphics projects.","tags":null,"title":"Byung Il Choi","type":"authors"},{"authors":["Byung Il Choi","Eric Shaffer (Advisor)"],"categories":null,"content":"Currently In Progress\n","date":1575158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575158400,"objectID":"445328289d8b39061d855be8ac8b7837","permalink":"/publication/proganterrain/","publishdate":"2019-12-01T00:00:00Z","relpermalink":"/publication/proganterrain/","section":"publication","summary":"Progressive Growing GAN used for Terrain Generation","tags":["AI","ProGAN","GAN","WGAN-GP","OpenGL"],"title":"Progressive Growing GAN for Terrain Generation","type":"publication"},{"authors":null,"categories":null,"content":"Introduction As terrain generation would require a resolution that is higher than 100, DCGAN may not be a good fit for the purpose. This project has adapted Progressive Growing GAN for stable training and generations in a higher resolution.\nProgressive Growing GAN Progressive Growing GAN uses He initialization, pixelwise normalization, minibatch standard deviation, equalized learning rate for convolutions, exponential running average, and blending layers with different resolutions. This fading technique helps the network to get stablized. The MNIST dataset was used in early stage to make sure that the network works well as shown in Misc section.\nTraining The neural network was trained on the topology data and surface color from NASA's Visible Earth.\nThe training process combines the colors of the earth's surface and it's topology, which enables the network to generate both colors and heights of a terrain like below:\n    generated terrain colors      generated heights   These generated samples can be visualized in 3D.\n3.1 Terrain Visualization in 3D The below video shows how the generated terrains can be visualized in 3D.\n  The generations from fixed random latent vectors were tracked, and their training progress in 3D representations were captured as below:\n3.2 Training Visualization in 3D We can see how the GAN is combining various features in the visualization, which we cannot see in the visualization of 2D.\n  3.2 Training Visualization in 2D The left half is how surface colors are generated and the right half shows the corresponding heights.\n  Acknowledgement This research project was advised by Prof. Eric Shaffer.\nMisc. MNIST digits generation using Progressive Growing GAN\n","date":1573948800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573948800,"objectID":"47e86858c4f5b6fa2b242b5054fcf196","permalink":"/project/terrain-gan/","publishdate":"2019-11-17T00:00:00Z","relpermalink":"/project/terrain-gan/","section":"project","summary":"An example of using the in-built project page.","tags":["AI","ProGAN","GAN","WGAN-GP","OpenGL","--"],"title":"Terrain Generation with GAN","type":"project"},{"authors":["Byung Il Choi","Cheng Xiang Zhai (Advisor)"],"categories":null,"content":"Currently In Progress\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"aa6ebe7f1a0f3948340266d610eb02fe","permalink":"/publication/gqe_timan/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/publication/gqe_timan/","section":"publication","summary":"Generative Query Expansion","tags":["Information Retrieval","Query Expansion","GQE"],"title":"Generative Query Expansion, Stochastic Approach with Quantitative Analysis","type":"publication"},{"authors":null,"categories":null,"content":"The project is in very early stage.\n","date":1548979200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548979200,"objectID":"3c5c2ca22e2d0f22b3bee1a6548d6ced","permalink":"/project/chessalphazero/","publishdate":"2019-02-01T00:00:00Z","relpermalink":"/project/chessalphazero/","section":"project","summary":"In Progress","tags":["AI","Reinforcement Learning","--"],"title":"Chess AI using AlphaZero (2020)","type":"project"},{"authors":null,"categories":null,"content":"Introduction The reinforcement learning has been around for more than a decade [2] , and Q-learning is one of most famous method for RL. DeepMind has especially utilized Q-learning, and evolved the idea to the next level by adapting non-linear model. Instead of Q table, Neural network is now used to predict the values, which is called Deep Q Network. The CNN and Deep Q-learning is what DeepMind’s work is all about.\nProblem DQN has to observe a game from ATARI and be trained to beat human level performance. Since Space Invader was the main example of early Deep-Mind’s paper, we are going to chose the game for this project. We are going to train our model to achieve the score of 500, which is the result of \u0026ldquo;Playing atari with deep reinforcement learn- ing.\u0026quot;, the first DeepMind’s paper. And Space Invader is a game that human level performance is hard to achieve in the early works of DQN.\nRelated Work DeepMind’s two early papers has two versions; we chose the version that uses three convolutional layers and one FC layer with 512 hidden units. This particular model for Space Invader has 6 outputs. All of the layers are followed by ReLU except the last output layer.\nTarget Network Because of the shifting weights of Q model, DeepMind has suggested to use two Q networks. This is related to divergence of Reinforcement Learning. The first network consistently updates as normal DNN would, but the second network, called the target network, doesn’t have sequential updates but is used to estimate the value of next state. Because of this it’s updated every 10,000 frames, but this can be changed through hyperparameter settings. According to the Deep-Mind’s paper, target network prevents of falling into a bad local minima and shifting weights. Thus, the target network has different set of parameters θ.\nExperience Replay Our problem includes correlations of sequences of input which consists of actions and states. In order to break this, DeepMind suggests to use Experience Replay. Experience Replay is another word for big sampling pool. The continuous sequences of data can be recorded to become a large pool, which can be used for sampling later. As we get more and more data, having correlation between sampled batch is more unlikely. Thereby, sampled batchs can be treated as if they are independent themselves.\nDQL Agent playing Space Invaders   Acknowledgement  This project was advised by Prof. Justin Sirignano. This project was tested and reviewed by Seongsu Ha  ","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"fea8bca9900f78441d6e9da047c827cc","permalink":"/project/dql-atari/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/project/dql-atari/","section":"project","summary":"An example of using the in-built project page.","tags":["DQL","Q Learning","AI","Deep Learning","--"],"title":"Deep Q-Learning for Atari Games","type":"project"},{"authors":null,"categories":null,"content":"Introduction An AI agent is often not tangible. It’s an artificially being that usually resides in virtual space. We can seldom see robots and drones with AI on media, but not usually in the physical world. It’s very hard to bring agents to the physical world, but if we let them stay in their space, it becomes a lot easier to interact. The VR technologies let us be in a virtual space, thus we can experience the embodiment of an AI agent. This project is motivated by this gateway to space of another being.\nProblem It’s very costly to bring AI to physical space, and it would cost a tremendous amount to develop and test such agent in physical space. But, if we are able to simulate good enough for AI agents, we believe that the cost of development can be reduced drastically. And, we believe the AI agent in VR can be very entertaining.\nPractices For comfortable experience, OBP suggests few things. And, these are the main guideline that we will follow:\n(i) distance to focusing object is between 0.75 and 3.5 meters [OBP 10],\n(ii) post-processing effects are applied to both eyes [OBP 10],\n(iii) use default FOV [OBP 13],\n(iv) Viewing the environment from a stationary position is most comfortable. [OBP 6] Users will mostly be standing in a training-stage, and evaluation stage in a game form would also mostly be in a stationary position.\nExceptions In case of having the network visualized as extra work, there may be a possibility of having frame-rate drops. But, this is only if we do the extra work.\n2 Q-Learning Agents Playing each other   Acknowledgement This project was advised by Prof. Eric Shaffer, and supervised by Jonathan Hoelzel.\n AI, Physics, and Graphics Programming by Byung Il Choi Graphics Programming and Game Play by Seongsu Ha User Interface and Graphic Design by Chris Wegenek Sound and Testing by Alejandro Marin  ","date":1545868800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545868800,"objectID":"0e31491c4fcb4615be5761f11e95cf90","permalink":"/project/q-learning-vr/","publishdate":"2018-12-27T00:00:00Z","relpermalink":"/project/q-learning-vr/","section":"project","summary":"To experience AI Agents in VR","tags":["AI","Reinforcement Learning","Unity","Q Learning","--"],"title":"Q-Learning Agent in VR","type":"project"},{"authors":null,"categories":null,"content":"Introduction In many query expansion studies, the primary evaluation depends on its search result. This project tries to provide a second way1 of measuring QE performance by measuring how many query expansion terms overlaps with ideal expansion terms, where ideal expansion terms are trained from a high performing QE from supervised learning.\nApproaches (i) Try to find expansion cadidates using GQE model which tries find the terms that maximizes the probability of coocurrening terms.\n(ii) Try to see if maximizing probabilities of coocurring terms is the right way\n(iii) Try to find cohesiveness of coocurring terms\nFinding Ideal Expansion Terms We need to find ideal expansion terms first, so we have a criterion for comparing model's ouput. A supervised learning can find expansion terms that performs search well as it gives a result with high P@10 and MAP. The ideal expansion from training on AP88/89 performs as following:\n   Expansion p@5 p@10 MAP@10 MAP@100 MAP@1k Recall@1k Recall@10k     +0 terms 0.41 0.371 0.28 \u0026lt; 0.25 \u0026lt; 0.25 0.662 0.861   +400 0.906 0.827 0.817 0.556 0.546 0.8578 0.9642   +2000 0.98 0.959 0.9832 0.8734 0.7935 0.8825 0.9495    In early work, the goal was to find 400-expansion terms. Later, we found that 400 was not the upper bound but 2000+ terms could be used for expansion.\nEarly Work The first approach assumes that 400 expansion terms are enough quantity for QE. The top-100 terms that maxmizes the probability of coocurrence decides high amount of MAP. So, finding these top-100 terms are important as top-100 contributes most to the maximization. GQE finds top 100 as below:\n   generated terrain colors   The above graph is in query level measurment which shows how many exansion terms could be found that are in top-100 ideal expansion terms.\nThe larger model finds more ideal expansion terms, but so is the false positive expansion candidates.\nQuality vs Quantity The early work tries find terms that maximizes the probability of coocurrence, and it focused on top 100 terms. During the process, we found that the number of expandable terms can be more than 1000 and it gives a much better result as we see below:\n   Expansion p@5 p@10 MAP@10 MAP@100 MAP@1k     +400 0.906 0.827 0.817 0.556 0.546   +2000 0.98 0.959 0.9832 0.8734 0.7935    Let's use Monte Carlo method to make sure that the quantity matters more than quality; we randomly pick n terms from the ideal expansion term pool that we got from supervised learning, and see how it performs below:\nRecent Work The idea of finding top 100 ideal expansion is not the right direction as we found that 400 ideal expansion terms were at its local maxima for its performance. The quantity of expansion term mattered more than the quality, where the quality is high when the ideal expansion terms maximizes the co-occurrence probability of the expansion terms.\nAcknowledgement This project is advised by Prof. Cheng Xiang Zhai\n  This project is currently under progress.\n   ","date":1510704000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1510704000,"objectID":"8f0ba94cc2e9eb6a61516188072bbe8b","permalink":"/project/gqe-timan/","publishdate":"2017-11-15T00:00:00Z","relpermalink":"/project/gqe-timan/","section":"project","summary":"Stochastic Approach and Quantitative Analysis","tags":["Information Retrieval","Query Expansion","BM25","--"],"title":"Generative Query Expansion","type":"project"},{"authors":null,"categories":null,"content":"1. Visualizing Mandelbrot set in realtime Mandelbrot set can be visualized using OpenGL and cuda together as above. If this is rendered using CPU1, its FPS is as below:\n   CPU usage   There is no parallelization with the base CPU model. Procedural double loop and heavy calculation is very slow that causes the rendering to become uninteractive. This is, however, very a good subject for the GPU parallel computation. Because, it has a small amount of branchings and rare memory access. Each pixel will have independency for computation in lots of cases.\n2. Using GPU for realtime visualization Converting double loop to a single loop is simple in case of rendering mandelbrot set, since PBO has a linear memory space. Similar to parallel histogram computation, the distrbution of computation can be easily done with GPU model.\nUsing proper amount of threads boosts the speed as following:\n   GPU usage   The left shows a kernel with 49x1024 threads, which boosts about 333 times compared to the base model. With proper thread-block dimension, 15% more improvement can be acheived.\n3. n-stream rendering The above kernel in section 2 renders about 42 pixels per thread 2, where each thread's memory access will be about 168 bytes apart with no global memory access. Even for writing, it is better to coalesce the memory access for a better cache use. Instead of reindexing the writing process, changing the calculation level to per-pixel base can help for speeding up the calculation with a parallelization. Using 42 streams with 376x128 threads can speed up even more as below:\n   GPU speedup with streams   This is about 650 times faster than the base CPU model. The use of shared memory could not speed up the process since the GPU model didn't depend on previously calculated values. The realtime visualization of Mandelbrot set was a problem for how to distribute the computation than how to optimize the kernel.\n4. Realtime Visualization The GPU model can be visualized in realtime as below:   Acknowledgement This project was advised by Prof. Klaus Mueller.\nFootnotes   using AMD 8300 in 2015\n using Nvidia 780 gtx\n   ","date":1430092800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1430092800,"objectID":"9d27af599fde21903978b103b17c05ba","permalink":"/project/mandelbrotcuda/","publishdate":"2015-04-27T00:00:00Z","relpermalink":"/project/mandelbrotcuda/","section":"project","summary":"Zooming into Mandelbrot set","tags":["Cuda","Fractal","Visualization","Graphics","Mandelbrot","--"],"title":"Realtime Fractal Zooming with CUDA","type":"project"},{"authors":null,"categories":null,"content":"2009 Finger Touch Tracking, R\u0026amp;D: Worked on a project for finger tracking with Bezier Forward Difference\n2008 Adjustable Game Probabilities with Constraints R\u0026amp;D: Worked on adjustable game probability in Impression Co.\n2006 Camera Work and Shader Coputation R\u0026amp;D: Worked on Camera Work related to curve tracking and Shader Computation\n2003 Scene Graph Library, Maya Plugin R\u0026amp;D: Worked on building scene graph library and related Maya Plugins in Cenozoic Co.\n2001 Skeleton Retargeting and Motion Capture Reprocessing R\u0026amp;D: Worked on motion captured data reprocessing in Cenozoic Co.\n1998 Text MUD Worked on Text MUD game for personal enjoyment\n","date":349401600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":349401600,"objectID":"10533d53f9e52a8f35bc58e449192bd1","permalink":"/project/etc/","publishdate":"1981-01-27T00:00:00Z","relpermalink":"/project/etc/","section":"project","summary":"Other old projects","tags":["project"],"title":"Etc","type":"project"}]