<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>byung.org</title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description>byung.org</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>some copyrights; (2015)</copyright><lastBuildDate>Sun, 01 Dec 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>byung.org</title>
      <link>/</link>
    </image>
    
    <item>
      <title>Progressive Growing GAN for Terrain Generation</title>
      <link>/publication/proganterrain/</link>
      <pubDate>Sun, 01 Dec 2019 00:00:00 +0000</pubDate>
      <guid>/publication/proganterrain/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;Currently In Progress&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Terrain Generation with GAN</title>
      <link>/project/terrain-gan/</link>
      <pubDate>Sun, 17 Nov 2019 00:00:00 +0000</pubDate>
      <guid>/project/terrain-gan/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;As terrain generation would require a resolution that is higher than 100, DCGAN may not be a good fit for the purpose. This project has adapted Progressive Growing GAN for stable training and generations in a higher resolution. For more detail, there&#39;s a technical &lt;a href=&#34;TerrainGenerationUsingPGAN.pdf&#34;&gt;draft&lt;/a&gt; available.&lt;/p&gt;
&lt;h2 id=&#34;progressive-growing-gan&#34;&gt;Progressive Growing GAN&lt;/h2&gt;
&lt;p&gt;Progressive Growing GAN uses He initialization, pixelwise normalization, minibatch standard deviation, equalized learning rate for convolutions, exponential running average, and blending layers with different resolutions. This fading technique helps the network to get stablized. The &lt;a href=&#34;/post/mnist-progan/&#34;&gt;MNIST&lt;/a&gt; dataset was used in early stage to make sure that the network works well as shown in Misc section.&lt;/p&gt;
&lt;h2 id=&#34;training&#34;&gt;Training&lt;/h2&gt;
&lt;p&gt;The neural network was trained on the topology data and surface color from NASA&#39;s &lt;a href=&#34;https://visibleearth.nasa.gov&#34;&gt;Visible Earth&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The training process combines the colors of the earth&#39;s surface and it&#39;s topology, which enables the network to generate both colors and heights of a terrain like below:&lt;/p&gt;
&lt;p&gt;












&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;featured.jpg&#34; data-caption=&#34;generated terrain colors&#34;&gt;
&lt;img src=&#34;featured.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    generated terrain colors
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;topo_example.jpg&#34; data-caption=&#34;generated heights&#34;&gt;
&lt;img src=&#34;topo_example.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    generated heights
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;These generated samples can be visualized in 3D.&lt;/p&gt;
&lt;h3 id=&#34;31-terrain-visualization-in-3d&#34;&gt;3.1 Terrain Visualization in 3D&lt;/h3&gt;
&lt;p&gt;The below video shows how the generated terrains can be visualized in 3D.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/WM52rjuR6Pw&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;The generations from fixed random latent vectors were tracked, and their training progress in 3D representations were captured as below:&lt;/p&gt;
&lt;h3 id=&#34;32-training-visualization-in-3d&#34;&gt;3.2 Training Visualization in 3D&lt;/h3&gt;
&lt;p&gt;We can see how the GAN is combining various features in the visualization, which we cannot see in the 2D visualization.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/rCjdObI5iFU&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h3 id=&#34;32-training-visualization-in-2d&#34;&gt;3.2 Training Visualization in 2D&lt;/h3&gt;
&lt;p&gt;The left half is how surface colors are generated and the right half shows the corresponding heights.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/0B4VAgV4Eyc&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h2 id=&#34;acknowledgement&#34;&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;This research project was advised by Prof. &lt;a href=&#34;https://cs.illinois.edu/directory/profile/shaffer1&#34;&gt;Eric Shaffer&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;misc&#34;&gt;Misc.&lt;/h2&gt;
&lt;p&gt;MNIST digits generation using Progressive Growing GAN&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ProGAN on Generating Digits from MNIST</title>
      <link>/post/mnist-progan/</link>
      <pubDate>Fri, 08 Nov 2019 00:00:00 +0000</pubDate>
      <guid>/post/mnist-progan/</guid>
      <description>&lt;p&gt;While building and testing a progressive growing GAN, I was able to train the network with the MNIST dataset. The results looks good and the training visualization shows how Progressive Growing GAN uses multiple layers to increase the resolution as below:&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/MlkW60qwY_Y&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h2 id=&#34;acknowledgement&#34;&gt;Acknowledgement&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;MNIST Dataset is from &lt;a href=&#34;http://yann.lecun.com/exdb/mnist/&#34;&gt;http://yann.lecun.com/exdb/mnist/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Progressive Growing GAN is implemented using &lt;a href=&#34;http://yann.lecun.com/exdb/mnist/&#34;&gt;PyTorch&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Generative Query Expansion, Stochastic Approach with Quantitative Analysis</title>
      <link>/publication/gqe_timan/</link>
      <pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate>
      <guid>/publication/gqe_timan/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;Currently In Progress&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Display Jupyter Notebooks with Academic</title>
      <link>/post/jupyter/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/post/jupyter/</guid>
      <description>&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; IPython.core.display &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; Image
Image(&lt;span style=&#34;color:#e6db74&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;./index_1_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;Welcome to Academic!&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Welcome to Academic!
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;install-python-and-jupyterlab&#34;&gt;Install Python and JupyterLab&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.anaconda.com/distribution/#download-section&#34;&gt;Install Anaconda&lt;/a&gt; which includes Python 3 and JupyterLab.&lt;/p&gt;
&lt;p&gt;Alternatively, install JupyterLab with &lt;code&gt;pip3 install jupyterlab&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;create-or-upload-a-jupyter-notebook&#34;&gt;Create or upload a Jupyter notebook&lt;/h2&gt;
&lt;p&gt;Run the following commands in your Terminal, substituting &lt;code&gt;&amp;lt;MY-WEBSITE-FOLDER&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;SHORT-POST-TITLE&amp;gt;&lt;/code&gt; with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;mkdir -p &amp;lt;MY-WEBSITE-FOLDER&amp;gt;/content/post/&amp;lt;SHORT-POST-TITLE&amp;gt;/
cd &amp;lt;MY-WEBSITE-FOLDER&amp;gt;/content/post/&amp;lt;SHORT-POST-TITLE&amp;gt;/
jupyter lab index.ipynb
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The &lt;code&gt;jupyter&lt;/code&gt; command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.&lt;/p&gt;
&lt;h2 id=&#34;edit-your-post-metadata&#34;&gt;Edit your post metadata&lt;/h2&gt;
&lt;p&gt;The first cell of your Jupter notebook will contain your post metadata (&lt;a href=&#34;https://sourcethemes.com/academic/docs/front-matter/&#34;&gt;front matter&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;In Jupter, choose &lt;em&gt;Markdown&lt;/em&gt; as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;---
title: My post&#39;s title
date: 2019-09-01

# Put any other Academic metadata here...
---
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Edit the metadata of your post, using the &lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content&#34;&gt;documentation&lt;/a&gt; as a guide to the available options.&lt;/p&gt;
&lt;p&gt;To set a &lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#featured-image&#34;&gt;featured image&lt;/a&gt;, place an image named &lt;code&gt;featured&lt;/code&gt; into your post&#39;s folder.&lt;/p&gt;
&lt;p&gt;For other tips, such as using math, see the guide on &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34;&gt;writing content with Academic&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;convert-notebook-to-markdown&#34;&gt;Convert notebook to Markdown&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;This post was created with Jupyter. The orginal files can be found at &lt;a href=&#34;https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter&#34;&gt;https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chess AI using AlphaZero (2020)</title>
      <link>/project/chessalphazero/</link>
      <pubDate>Fri, 01 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/project/chessalphazero/</guid>
      <description>&lt;p&gt;The project is in a very early stage.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep Q-Learning for Atari Games</title>
      <link>/project/dql-atari/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>/project/dql-atari/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The reinforcement learning has been around for more than a decade [2] , and Q-learning is one of most famous method for RL. DeepMind has especially utilized Q-learning, and evolved the idea to the next level by adapting non-linear model. Instead of Q table, Neural network is now used to predict the values, which is called Deep Q Network. The CNN and Deep Q-learning is what DeepMind’s work is all about.&lt;/p&gt;
&lt;h2 id=&#34;problem&#34;&gt;Problem&lt;/h2&gt;
&lt;p&gt;DQN has to observe a game from ATARI and be trained to beat human level performance. Since Space Invader was the main example of early Deep-Mind’s paper, we are going to chose the game for this project. We are going to train our model to achieve the score of 500, which is the result of &amp;ldquo;Playing atari with deep reinforcement learn-
ing.&amp;quot;, the first DeepMind’s paper. And Space Invader is a game that human level performance is hard to achieve in the early works of DQN.&lt;/p&gt;
&lt;h2 id=&#34;related-work&#34;&gt;Related Work&lt;/h2&gt;
&lt;p&gt;DeepMind’s two early papers has two versions; we chose the version that uses three convolutional layers and one FC layer with 512 hidden units. This particular model for Space Invader has 6 outputs. All of the layers are followed by ReLU except the last output layer.&lt;/p&gt;
&lt;h2 id=&#34;target-network&#34;&gt;Target Network&lt;/h2&gt;
&lt;p&gt;Because of the shifting weights of Q model, DeepMind has suggested to use two Q networks. This is related to divergence of Reinforcement Learning. The first network consistently updates as normal DNN would, but the second network, called the target network, doesn’t have sequential updates but is used to estimate the value of next state. Because of this it’s updated every 10,000 frames, but this can be changed through hyperparameter settings. According to the Deep-Mind’s paper, target network prevents of falling into a bad local minima and shifting weights. Thus, the target network has different set of parameters θ.&lt;/p&gt;
&lt;h2 id=&#34;experience-replay&#34;&gt;Experience Replay&lt;/h2&gt;
&lt;p&gt;Our problem includes correlations of sequences of input which consists of actions and states. In order to break
this, DeepMind suggests to use Experience Replay. Experience Replay is another word for big sampling pool. The
continuous sequences of data can be recorded to become a large pool, which can be used for sampling later. As we get more and more data, having correlation between sampled batch is more unlikely. Thereby, sampled batchs can be treated as if they are independent themselves.&lt;/p&gt;
&lt;h2 id=&#34;dql-agent-playing-space-invaders&#34;&gt;DQL Agent playing Space Invaders&lt;/h2&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/OiTcfCf7Z-E&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h2 id=&#34;acknowledgement&#34;&gt;Acknowledgement&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;This project was advised by Prof. &lt;a href=&#34;https://jasirign.github.io&#34;&gt;Justin Sirignano&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;This project was tested and reviewed by Seongsu Ha&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Q-Learning Agent in VR</title>
      <link>/project/q-learning-vr/</link>
      <pubDate>Thu, 27 Dec 2018 00:00:00 +0000</pubDate>
      <guid>/project/q-learning-vr/</guid>
      <description>&lt;h2 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;An AI agent is often not tangible. It’s an artificially being that usually resides in virtual space. We can seldom see robots and drones with AI on media, but not usually in the physical world. It’s very hard to bring agents to the physical world, but if we let them stay in their space, it becomes a lot easier to interact. The VR technologies let us be in a virtual space, thus we can experience the embodiment of an AI agent. This project is motivated by this gateway to space of another being.&lt;/p&gt;
&lt;h2 id=&#34;2-problem&#34;&gt;2. Problem&lt;/h2&gt;
&lt;p&gt;It’s very costly to bring AI to physical space, and it would cost a tremendous amount to develop and test such agent in physical space. But, if we are able to simulate good enough for AI agents, we believe that the cost of development can be reduced drastically. And, we believe the AI agent in VR can be very entertaining.&lt;/p&gt;
&lt;h2 id=&#34;3-practices&#34;&gt;3. Practices&lt;/h2&gt;
&lt;p&gt;For comfortable experience, OBP suggests few things. And, these are the main guideline that we will follow:&lt;br&gt;
(i) distance to focusing object is between 0.75 and 3.5 meters [OBP 10],&lt;br&gt;
(ii) post-processing effects are applied to both eyes [OBP 10],&lt;br&gt;
(iii) use default FOV [OBP 13],&lt;br&gt;
(iv) Viewing the environment from a stationary position is most comfortable. [OBP 6] Users will mostly be standing in a training-stage, and evaluation stage in a game form would also mostly be in a stationary position.&lt;/p&gt;
&lt;h2 id=&#34;4-rules-for-agents&#34;&gt;4. Rules for Agents&lt;/h2&gt;
&lt;p&gt;All types of agents have 30 decisions/sec and moves in same speed except human players.
When the ball bounces, its reflection angle and speed randomly change, so that its harder for the AI agents. Without continuous random adjustments, Q-Agents can play the game for very long time. It&#39;s designed to test their limits.&lt;/p&gt;
&lt;h3 id=&#34;51-qlearning-agents-playing-each-other&#34;&gt;5-1 Q-Learning Agents Playing each other&lt;/h3&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/3fCz_Gp1MB4&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;Most of the mistakes are by movements that are off by an inch. The high ball speed can reduce the game time effectively.&lt;/p&gt;
&lt;h3 id=&#34;52-qagent-vs-greedy-agent&#34;&gt;5-2 Q-Agent vs Greedy Agent&lt;/h3&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/-JLkZ-1K8n4&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;A greedy agent can never beat a Q-Learning agent, even though they have same decision making rates and moving speed.&lt;/p&gt;
&lt;h2 id=&#34;acknowledgement&#34;&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;This project was advised by Prof. &lt;a href=&#34;https://cs.illinois.edu/directory/profile/shaffer1&#34;&gt;Eric Shaffer&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AI, Physics, and Graphics Programming by Byung Il Choi&lt;/li&gt;
&lt;li&gt;Graphics Programming and Game Play by Seongsu Ha&lt;/li&gt;
&lt;li&gt;User Interface and Graphic Design by Chris Wegenek&lt;/li&gt;
&lt;li&gt;Sound and Testing by Alejandro Marin&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Generative Query Expansion</title>
      <link>/project/gqe-timan/</link>
      <pubDate>Wed, 15 Nov 2017 00:00:00 +0000</pubDate>
      <guid>/project/gqe-timan/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In many query expansion studies, the primary evaluation depends on its search result. This project tries to provide a second way&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; of measuring QE performance by measuring how many query expansion terms overlaps with ideal expansion terms, where ideal expansion terms are trained from a high performing QE from supervised learning.&lt;/p&gt;
&lt;h2 id=&#34;approaches&#34;&gt;Approaches&lt;/h2&gt;
&lt;p&gt;(i) Try to find expansion cadidates using GQE model which tries find the terms that maximizes the probability of coocurrening terms.&lt;br&gt;
(ii) Try to see if maximizing probabilities of coocurring terms is the right way&lt;br&gt;
(iii) Try to find cohesiveness of coocurring terms&lt;/p&gt;
&lt;h2 id=&#34;finding-ideal-expansion-terms&#34;&gt;Finding Ideal Expansion Terms&lt;/h2&gt;
&lt;p&gt;We need to find ideal expansion terms first, so we have a criterion for comparing model&#39;s ouput. A supervised learning can find expansion terms that performs search well as it gives a result with high P@10 and MAP.
The ideal expansion from training on AP88/89 performs as following:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Expansion&lt;/th&gt;
&lt;th&gt;p@5&lt;/th&gt;
&lt;th&gt;p@10&lt;/th&gt;
&lt;th&gt;MAP@10&lt;/th&gt;
&lt;th&gt;MAP@100&lt;/th&gt;
&lt;th&gt;MAP@1k&lt;/th&gt;
&lt;th&gt;Recall@1k&lt;/th&gt;
&lt;th&gt;Recall@10k&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;+0 terms&lt;/td&gt;
&lt;td&gt;0.41&lt;/td&gt;
&lt;td&gt;0.371&lt;/td&gt;
&lt;td&gt;0.28&lt;/td&gt;
&lt;td&gt;&amp;lt; 0.25&lt;/td&gt;
&lt;td&gt;&amp;lt; 0.25&lt;/td&gt;
&lt;td&gt;0.662&lt;/td&gt;
&lt;td&gt;0.861&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;+400&lt;/td&gt;
&lt;td&gt;0.906&lt;/td&gt;
&lt;td&gt;0.827&lt;/td&gt;
&lt;td&gt;0.817&lt;/td&gt;
&lt;td&gt;0.556&lt;/td&gt;
&lt;td&gt;0.546&lt;/td&gt;
&lt;td&gt;0.8578&lt;/td&gt;
&lt;td&gt;0.9642&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;+2000&lt;/td&gt;
&lt;td&gt;0.98&lt;/td&gt;
&lt;td&gt;0.959&lt;/td&gt;
&lt;td&gt;0.9832&lt;/td&gt;
&lt;td&gt;0.8734&lt;/td&gt;
&lt;td&gt;0.7935&lt;/td&gt;
&lt;td&gt;0.8825&lt;/td&gt;
&lt;td&gt;0.9495&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In early work, the goal was to find 400-expansion terms. Later, we found that 400 was not the upper bound but 2000+ terms could be used for expansion.&lt;/p&gt;
&lt;h2 id=&#34;early-work&#34;&gt;Early Work&lt;/h2&gt;
&lt;p&gt;The first approach assumes that 400 expansion terms are enough quantity for QE. The top-100 terms that maxmizes the probability of coocurrence decides high amount of MAP. So, finding these top-100 terms are important as top-100 contributes most to the maximization. GQE finds top 100 as below:&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;result_early.jpg&#34; data-caption=&#34;generated terrain colors&#34;&gt;
&lt;img src=&#34;result_early.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    generated terrain colors
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The above graph is in query level measurment which shows how many exansion terms could be found that are in top-100 ideal expansion terms.&lt;/p&gt;
&lt;p&gt;The larger model finds more ideal expansion terms, but so is the false positive expansion candidates.&lt;/p&gt;
&lt;h2 id=&#34;quality-vs-quantity&#34;&gt;Quality vs Quantity&lt;/h2&gt;
&lt;p&gt;The early work tries find terms that maximizes the probability of coocurrence, and it focused on top 100 terms. During the process, we found that the number of expandable terms can be more than 1000 and it gives a much better result as we see below:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Expansion&lt;/th&gt;
&lt;th&gt;p@5&lt;/th&gt;
&lt;th&gt;p@10&lt;/th&gt;
&lt;th&gt;MAP@10&lt;/th&gt;
&lt;th&gt;MAP@100&lt;/th&gt;
&lt;th&gt;MAP@1k&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;+400&lt;/td&gt;
&lt;td&gt;0.906&lt;/td&gt;
&lt;td&gt;0.827&lt;/td&gt;
&lt;td&gt;0.817&lt;/td&gt;
&lt;td&gt;0.556&lt;/td&gt;
&lt;td&gt;0.546&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;+2000&lt;/td&gt;
&lt;td&gt;0.98&lt;/td&gt;
&lt;td&gt;0.959&lt;/td&gt;
&lt;td&gt;0.9832&lt;/td&gt;
&lt;td&gt;0.8734&lt;/td&gt;
&lt;td&gt;0.7935&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Let&#39;s use Monte Carlo method to make sure that the quantity matters more than quality; we randomly pick n terms from the ideal expansion term pool that we got from supervised learning, and see how it performs below:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Max Cooc 400&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Monte Carlo @400&lt;/th&gt;
&lt;th&gt;Monte Carlo @500&lt;/th&gt;
&lt;th&gt;Monte Carlo @600&lt;/th&gt;
&lt;th&gt;Monte Carlo @700&lt;/th&gt;
&lt;th&gt;Monte Carlo @800&lt;/th&gt;
&lt;th&gt;Monte Carlo @900&lt;/th&gt;
&lt;th&gt;Monte Carlo @1000&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;MAP@100:&lt;/td&gt;
&lt;td&gt;0.556&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;0.568&lt;/td&gt;
&lt;td&gt;0.614&lt;/td&gt;
&lt;td&gt;0.653&lt;/td&gt;
&lt;td&gt;0.682&lt;/td&gt;
&lt;td&gt;0.704&lt;/td&gt;
&lt;td&gt;0.725&lt;/td&gt;
&lt;td&gt;0.742&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Max Cooc is using 400 expansion terms which come from maximized term co-occurence probability; this is no better than QE with 400 randomly picked expansion terms from ideal expansion pool as shown in Monte Carlo@400 method. As random picks are better than the co-occurence proability maximization method, it&#39;s better to focus on finding terms quantitatively.&lt;/p&gt;
&lt;h2 id=&#34;recent-work&#34;&gt;Recent Work&lt;/h2&gt;
&lt;p&gt;The idea of finding top 100 ideal expansion is not the right direction as we found that 400 ideal expansion terms were at its local maxima for its performance. The quantity of expansion term mattered more than the quality, where the quality is high when the ideal expansion terms maximizes the co-occurrence probability of the expansion terms.&lt;/p&gt;
&lt;p&gt;GQE seems to capture a lot of ideal expansion terms:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;GQE first n-terms&lt;/th&gt;
&lt;th&gt;0-50&lt;/th&gt;
&lt;th&gt;0-100&lt;/th&gt;
&lt;th&gt;0-200&lt;/th&gt;
&lt;th&gt;0-300&lt;/th&gt;
&lt;th&gt;0-400&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Avg ideal terms found&lt;/td&gt;
&lt;td&gt;41&lt;/td&gt;
&lt;td&gt;82&lt;/td&gt;
&lt;td&gt;161&lt;/td&gt;
&lt;td&gt;236&lt;/td&gt;
&lt;td&gt;304&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Precision&lt;/td&gt;
&lt;td&gt;0.82&lt;/td&gt;
&lt;td&gt;0.82&lt;/td&gt;
&lt;td&gt;0.805&lt;/td&gt;
&lt;td&gt;07867&lt;/td&gt;
&lt;td&gt;0.76&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The precision of finding these ideal expansion terms with GQE is quite high, but its error is not low enough yet. But, with appropriate terms injected, GQE&#39;s extension can be useful. In the first feedback loop, we get enough positive terms that can lower the error rates of GQE. With one feedback and GQE expansion, the search results improves as below:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;No GQE&lt;/th&gt;
&lt;th&gt;GQE+50&lt;/th&gt;
&lt;th&gt;GQE+100&lt;/th&gt;
&lt;th&gt;GQE+150&lt;/th&gt;
&lt;th&gt;GQE+200&lt;/th&gt;
&lt;th&gt;GQE+250&lt;/th&gt;
&lt;th&gt;GQE+300&lt;/th&gt;
&lt;th&gt;GQE+350&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;p@5&lt;/td&gt;
&lt;td&gt;0.41&lt;/td&gt;
&lt;td&gt;0.496&lt;/td&gt;
&lt;td&gt;0.506&lt;/td&gt;
&lt;td&gt;0.516&lt;/td&gt;
&lt;td&gt;0.522&lt;/td&gt;
&lt;td&gt;0.526&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0.534&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;0.514&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;p@10&lt;/td&gt;
&lt;td&gt;0.371&lt;/td&gt;
&lt;td&gt;0.391&lt;/td&gt;
&lt;td&gt;0.404&lt;/td&gt;
&lt;td&gt;0.411&lt;/td&gt;
&lt;td&gt;0.417&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0.428&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;0.42&lt;/td&gt;
&lt;td&gt;0.415&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;GQE+n is for queries expanded with n-GQE terms and 1 feedback. The injection of true positive terms from a feedback helps to reduce GQE&#39;s error rate, and improves the search performance.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;So far, we&#39;ve found that GQE captures a great portion of ideal expansion terms and is useful when it&#39;s used with the first feedback. If we find a way to increase the precision of finding GQE candidates(ie. filter out false positive terms), then the query expansion will improve even more as GQE&#39;s candidate terms are becoming closer to ideal expansion terms.&lt;/p&gt;
&lt;h2 id=&#34;acknowledgement&#34;&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;This project is advised by Prof. &lt;a href=&#34;http://czhai.cs.illinois.edu&#34;&gt;Cheng Xiang Zhai&lt;/a&gt;&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;This project is currently under progress.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Realtime Fractal Zooming with CUDA</title>
      <link>/project/mandelbrotcuda/</link>
      <pubDate>Mon, 27 Apr 2015 00:00:00 +0000</pubDate>
      <guid>/project/mandelbrotcuda/</guid>
      <description>&lt;h2 id=&#34;1-visualizing-mandelbrot-set-in-realtime&#34;&gt;1. Visualizing Mandelbrot set in realtime&lt;/h2&gt;
&lt;p&gt;Mandelbrot set can be visualized using OpenGL and cuda together as above. If this is rendered using CPU&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, its FPS is as below:&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;figure1.jpg&#34; data-caption=&#34;CPU usage&#34;&gt;
&lt;img src=&#34;figure1.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    CPU usage
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;There is no parallelization with the base CPU model. Procedural double loop and heavy calculation is very slow that causes the rendering to become uninteractive. This is, however, very a good subject for the GPU parallel computation. Because, it has a small amount of branchings and rare memory access. Each pixel will have independency for computation in lots of cases.&lt;/p&gt;
&lt;h2 id=&#34;2-using-gpu-for-realtime-visualization&#34;&gt;2. Using GPU for realtime visualization&lt;/h2&gt;
&lt;p&gt;Converting double loop to a single loop is simple in case of rendering mandelbrot set, since PBO has a linear memory space. Similar to parallel histogram computation, the distrbution of computation can be easily done with GPU model.&lt;/p&gt;
&lt;p&gt;Using proper amount of threads boosts the speed as following:&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;figure2.jpg&#34; data-caption=&#34;GPU usage&#34;&gt;
&lt;img src=&#34;figure2.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    GPU usage
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The left shows a kernel with 49x1024 threads, which boosts about 333 times compared to the base model. With proper thread-block dimension, 15% more improvement can be acheived.&lt;/p&gt;
&lt;h2 id=&#34;3-nstream-rendering&#34;&gt;3. n-stream rendering&lt;/h2&gt;
&lt;p&gt;The above kernel in section 2 renders about 42 pixels per thread &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;, where each thread&#39;s memory access will be about 168 bytes apart with no global memory access. Even for writing, it is better to coalesce the memory access for a better cache use. Instead of reindexing the writing process, changing the calculation level to per-pixel base can help for speeding up the calculation with a parallelization. Using 42 streams with 376x128 threads can speed up even more as below:&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;figure3.jpg&#34; data-caption=&#34;GPU speedup with streams&#34;&gt;
&lt;img src=&#34;figure3.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    GPU speedup with streams
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;This is about 650 times faster than the base CPU model. The use of shared memory could not speed up the process since the GPU model didn&#39;t depend on previously calculated values. The realtime visualization of Mandelbrot set was a problem for how to distribute the computation than how to optimize the kernel.&lt;/p&gt;
&lt;h2 id=&#34;4-realtime-visualization&#34;&gt;4. Realtime Visualization&lt;/h2&gt;
&lt;p&gt;The GPU model can be visualized in realtime as below:

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/rqsVY-9iMKo&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;h2 id=&#34;acknowledgement&#34;&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;This project was advised by Prof. &lt;a href=&#34;https://www3.cs.stonybrook.edu/~mueller/&#34;&gt;Klaus Mueller&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;footnotes&#34;&gt;Footnotes&lt;/h2&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;using AMD 8300 in 2015&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;using Nvidia 780 gtx&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Etc</title>
      <link>/project/etc/</link>
      <pubDate>Tue, 27 Jan 1981 00:00:00 +0000</pubDate>
      <guid>/project/etc/</guid>
      <description>&lt;h3 id=&#34;2009-finger-touch-tracking&#34;&gt;2009 Finger Touch Tracking,&lt;/h3&gt;
&lt;p&gt;R&amp;amp;D: Worked on a project for finger tracking with Bezier Forward Difference&lt;/p&gt;
&lt;h3 id=&#34;2008-adjustable-game-probabilities-with-constraints&#34;&gt;2008 Adjustable Game Probabilities with Constraints&lt;/h3&gt;
&lt;p&gt;R&amp;amp;D: Worked on adjustable game probability in Impression Co.&lt;/p&gt;
&lt;h3 id=&#34;2006-camera-work-and-shader-coputation&#34;&gt;2006 Camera Work and Shader Coputation&lt;/h3&gt;
&lt;p&gt;R&amp;amp;D: Worked on Camera Work related to curve tracking and Shader Computation&lt;/p&gt;
&lt;h3 id=&#34;2003-scene-graph-library-maya-plugin&#34;&gt;2003 Scene Graph Library, Maya Plugin&lt;/h3&gt;
&lt;p&gt;R&amp;amp;D: Worked on building scene graph library and related Maya Plugins in Cenozoic Co.&lt;/p&gt;
&lt;h3 id=&#34;2001-skeleton-retargeting-and-motion-capture-reprocessing&#34;&gt;2001 Skeleton Retargeting and Motion Capture Reprocessing&lt;/h3&gt;
&lt;p&gt;R&amp;amp;D: Worked on motion captured data reprocessing in Cenozoic Co.&lt;/p&gt;
&lt;h3 id=&#34;1998-text-mud&#34;&gt;1998 Text MUD&lt;/h3&gt;
&lt;p&gt;Worked on Text MUD game for personal enjoyment&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
